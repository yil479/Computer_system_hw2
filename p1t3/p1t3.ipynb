{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import lower\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format('xml').options(rowTag='page').load('hdfs:/enwiki_small.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- redirect: string (nullable = true)\n",
      " |-- restrictions: string (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _deleted: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- minor: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _xml:space: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(text):\n",
    "    title_lst = []\n",
    "    text_lower = text['text']['_VALUE']\n",
    "    #Titles = re.findall(r'\\[\\[(.*?)\\]\\]',text_lower)\n",
    "    if text_lower is not None:\n",
    "        #convert text to lower cases\n",
    "        Titles = re.findall(r'\\[\\[(.*?)\\]\\]',text_lower.lower())\n",
    "        for title in Titles:\n",
    "            if \"#\" not in title:\n",
    "                if \":\" in title:\n",
    "                    if \"category:\" in title:\n",
    "                        #strop all title start with empty\n",
    "                        trim_title=title.split('|')[0].strip()\n",
    "                        title_lst.append(trim_title)\n",
    "                else:\n",
    "                    trim_title=title.split('|')[0].strip()\n",
    "                    title_lst.append(trim_title)\n",
    "    return title_lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PairLinks(links):\n",
    "    return links[0], links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "title_list = udf(lambda y: get_titles(y), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_list = title_list(col('revision'))\n",
    "df2 = df.withColumn(\"revisions\",revision_list)\n",
    "\n",
    "df2_table = explode(df2.revisions)\n",
    "article_table = df2.select(lower(col('title')),df2_table)\n",
    "article_table= article_table.orderBy(\"title\",\"col\",ascending=True)\n",
    "article_table = article_table.select(col(\"lower(title)\").alias(\"article_name\"), col(\"col\").alias(\"links\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|    article_name|               links|\n",
      "+----------------+--------------------+\n",
      "|\"love and theft\"|                2001|\n",
      "|\"love and theft\"|accidents & accus...|\n",
      "|\"love and theft\"|           accordion|\n",
      "|\"love and theft\"|            allmusic|\n",
      "|\"love and theft\"|   americana (music)|\n",
      "|\"love and theft\"|anthology of amer...|\n",
      "|\"love and theft\"|        augie meyers|\n",
      "|\"love and theft\"|        augie meyers|\n",
      "|\"love and theft\"|               banjo|\n",
      "|\"love and theft\"|         bass guitar|\n",
      "|\"love and theft\"|       billboard 200|\n",
      "|\"love and theft\"|       billboard 200|\n",
      "|\"love and theft\"|  blender (magazine)|\n",
      "|\"love and theft\"|               blues|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "|\"love and theft\"|           bob dylan|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate contritbuions with corresponding rank for all neighbors\n",
    "def Contribution(neighbor, rank):\n",
    "    for links in neighbor:\n",
    "        yield (links, rank / len(neighbor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert t2 to rdd form table with title and internal links\n",
    "neighbor_article = article_table.rdd\n",
    "#for each title find their neighbors\n",
    "neighbor_table = lines.map(lambda links: PairLinks(links)).distinct().groupByKey().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_table = neighbor_table.map(lambda neighbors: (neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(10):\n",
    "    #contributions\n",
    "    contributions = neighbor_table.join(rank_table).flatMap(lambda neighbor_rank: Contribution(neighbor_rank[1][0], neighbor_rank[1][1]))\n",
    "    #ranks\n",
    "    rank_table = contributions.reduceByKey(add).mapValues(lambda article_rank: article_rank * 0.85 + 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert rdd form back to dataframe\n",
    "final_table = rank_table.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order by title and rank\n",
    "final_table= final_table.orderBy(\"_1\",\"_2\",ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                  _1|                 _2|\n",
      "+--------------------+-------------------+\n",
      "|                    |0.16530696425076816|\n",
      "|                   !|0.15490812663468756|\n",
      "|                 !!!| 0.1506720648853998|\n",
      "|          !karapuri!|0.15260782274534734|\n",
      "|      !kung language| 0.1512953660490697|\n",
      "|        !kung people|0.15047510351721316|\n",
      "|             !wowow!|0.15027220168350128|\n",
      "|                !x처천|0.15545032056327937|\n",
      "|       !x처천 language| 0.1533434083318205|\n",
      "|                   \"|0.15052328784593352|\n",
      "|                 \"*\"|0.15141240303084214|\n",
      "|                 \"+\"|0.15141240303084214|\n",
      "|  \"a piece of steak\"|0.15140337745780408|\n",
      "|            \"a+0(m)\"| 0.1512858454888697|\n",
      "|    \"broadway micky\"|0.15108184684160522|\n",
      "|  \"crocodile\" dundee|0.15061857758320338|\n",
      "|\"do not disturb\" ...|0.15035621269746224|\n",
      "|\"dr. death\" steve...| 0.1509905725621399|\n",
      "| \"einstein\" anderson| 0.1529306624230988|\n",
      "|  \"from hell\" letter|0.15102813279112887|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD reference: https://tinyurl.com/yc48trsk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
